{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhXAxvxQQKj3"
      },
      "source": [
        "<img src='https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/assets/images/logo.png?raw=true'>\n",
        "\n",
        "\n",
        "## üëãüèΩ What's up! It's [Harpreet](https://twitter.com/DataScienceHarp)\n",
        "\n",
        "Welcome to this tutorial notebook on [DataGradients](https://github.com/Deci-AI/data-gradients) for **object detection datasets**. DG is an open-source Python library, created by [Deci AI](https://deci.ai/),  for computer vision dataset analysis. If you're looking for the **segmentation datasets** notebook, you can find that [here.](https://bit.ly/dg-starter-notebook-seg)\n",
        "\n",
        "I'll be guiding you through this notebook. At any point, if you get stuck or have questions, feel free to [read the docs](https://bit.ly/dg-docs) or get in touch:\n",
        "\n",
        "1) Send me an email with your issue: harpreet.sahota@deci.ai\n",
        "\n",
        "2) Hop into the [Deep Learning Daily (powered by Deci) Discord server](https://discord.gg/p9ecgRhDR8), and let me know what your question is.\n",
        "\n",
        "3) [Open an issue on GitHub](https://github.com/Deci-AI/data-gradients/issues/new)\n",
        "\n",
        "Let's get to it...\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Whether you're working on image classification, object detection, or [semantic segmentation](https://bit.ly/dg-starter-notebook-seg), DataGradients helps you gain insights and analyze your datasets effectively.\n",
        "\n",
        "In this tutorial, you'll explore the features and functionalities of DataGradients, guiding you through comprehensive data analysis for computer vision projects.\n",
        "\n",
        "With DataGradients, you can:\n",
        "\n",
        "- Analyze image features such as color distribution, brightness, and size.\n",
        "- Profile object detection datasets with metrics like bounding box area, intersection, and class frequency.\n",
        "- Understand segmentation datasets using object area, width, height, and class frequency.\n",
        "- Visualize samples for a better understanding.\n",
        "- And [much more](https://github.com/Deci-AI/data-gradients/blob/master/documentation/feature_description.md)\n",
        "\n",
        "Profiling your datasets has never been easier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH6JJwsPSNB4"
      },
      "source": [
        "## üë®üèΩ‚Äçüîß Step 0: Installation\n",
        "\n",
        "Note: after installation is complete you will need to restart this notebook. Do the following: `Runtime -> Restart runtime`.\n",
        "\n",
        "Be careful NOT to select `Disconnect and delete runtime`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MdOWVad76Se"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# data-gradients\n",
        "!pip install data-gradients\n",
        "\n",
        "# to get data from roboflow\n",
        "!pip install roboflow\n",
        "\n",
        "# for displaying pdfs as images in notebook\n",
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "# for pretty printing json\n",
        "!pip install Pygments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Utility functions\n",
        "\n",
        "The `display_pdf_pages` function is a utility that displays each page of a PDF file as images in separate output cells.\n",
        "\n",
        "Given the path to a PDF file, it converts the PDF into a list of PIL Images using `pdf2image`.\n",
        "\n",
        "It then iterates through the list and displays each image using `IPython.display`.\n",
        "\n",
        "This function is useful for visually examining PDF content, such as reviewing pages or checking layouts."
      ],
      "metadata": {
        "id": "IkqllnKe5VFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "from IPython.display import display\n",
        "\n",
        "def display_pdf_pages(pdf_path):\n",
        "    \"\"\"\n",
        "    Display each page of a PDF file as images in separate output cells.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified PDF file is not found.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert PDF to a list of PIL Images\n",
        "        images = convert_from_path(pdf_path)\n",
        "\n",
        "        # Display each image\n",
        "        for i, image in enumerate(images):\n",
        "            # Display the image\n",
        "            display(image)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The specified PDF file was not found.\")"
      ],
      "metadata": {
        "id": "9q9jmV0H5NA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `print_pretty_json` function opens a JSON file, formats the data with an indent of 4 spaces using `json.dumps`, applies syntax highlighting with `Pygments`, and prints the pretty-printed JSON data in the cell below."
      ],
      "metadata": {
        "id": "D-vqYR4kLoGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pygments import highlight, lexers, formatters\n",
        "from pathlib import Path\n",
        "\n",
        "def print_pretty_json(file_path):\n",
        "    \"\"\"\n",
        "    Function to pretty print a JSON file with colorization.\n",
        "\n",
        "    Args:\n",
        "        file_path (Union[str, Path]): The path of the JSON file to be pretty-printed.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the file at `file_path` doesn't exist.\n",
        "        json.JSONDecodeError: If the file at `file_path` is not valid JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the file\n",
        "        with open(file_path, 'r') as f:\n",
        "            # Load the JSON data from the file\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise json.JSONDecodeError(\"Invalid JSON file\", \"\", 0)\n",
        "\n",
        "    # Pretty print the JSON data with an indent of 4 spaces\n",
        "    formatted_json = json.dumps(data, indent=4)\n",
        "\n",
        "    # Colorize the pretty-printed JSON data\n",
        "    colorful_json = highlight(formatted_json,\n",
        "                              lexers.JsonLexer(),\n",
        "                              formatters.TerminalFormatter()\n",
        "                              )\n",
        "\n",
        "    # Print the colorized, pretty-printed JSON data\n",
        "    print(colorful_json)\n"
      ],
      "metadata": {
        "id": "xoEhr6LoLgqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIc3dl6XSWTI"
      },
      "source": [
        "## ‚§µÔ∏è Step 1: Download Dataset\n",
        "\n",
        "Before you start, you need to create a Roboflow [account and get your API key](https://app.roboflow.com/login). If you're not sure how to find your API key, [here's how](https://www.loom.com/share/05277274e8d542efaf9bc3f33c1396d3?sid=3a41d4c5-c0c7-4712-bf4b-6a8c7ba51947)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcBFKcjV8FNb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"<your-roboflow-key-here>\")\n",
        "project = rf.workspace(\"joseph-nelson\").project(\"uno-cards\")\n",
        "dataset = project.version(1).download(\"yolov8\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following will open the `data.yaml`  file that comes with the dataset, read that yaml's contents, and converts those contents into a Python dictionary using the `safe_load` function from the `yaml` module."
      ],
      "metadata": {
        "id": "1VKqcnuDNjgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# define the path to your YAML file\n",
        "yaml_file_path = '/content/Uno-Cards-1/data.yaml'\n",
        "\n",
        "# open the YAML file and load it into a dictionary\n",
        "with open(yaml_file_path, 'r') as f:\n",
        "    data_yaml = yaml.safe_load(f)"
      ],
      "metadata": {
        "id": "GrjPPEOd-np5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7dvrawUUrD7"
      },
      "source": [
        "## üíæ Step 2: Instantiating Dataloaders\n",
        "\n",
        "To use DataGradients, you need to be equipped with a few prerequisites:\n",
        "\n",
        "- **Dataset**: Includes a Train set and a Validation or a Test set.\n",
        "\n",
        "- **Class Names**: A list of the unique categories present in your dataset.\n",
        "\n",
        "- **Iterable**: A method to iterate over your Dataset providing images and labels. This can be any of the following:\n",
        "  - PyTorch Dataloader\n",
        "  - PyTorch Dataset\n",
        "  - Generator that yields image/label pairs\n",
        "  - Any other iterable you use for model training/validation\n",
        "\n",
        "DataGradients provides some functionality that allows you to easily load your data without any extra coding needed:\n",
        "- `YoloFormatDetectionDataset`\n",
        "- `VOCFormatDetectionDataset`\n",
        "- `VOCDetectionDataset`\n",
        "\n",
        "You can learn more about those [here.](https://github.com/Deci-AI/data-gradients/blob/master/documentation/datasets.md)\n",
        "\n",
        "\n",
        "#### üö® If your dataset and annotations is in a custom format, you can use DataGradients Dataset Adapters. Learn more about those [here](https://github.com/Deci-AI/data-gradients#dataset-adapters).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_params = {\n",
        "    'data_dir':'/content/Uno-Cards-1',\n",
        "    'train_images_dir':'train/images',\n",
        "    'train_labels_dir':'train/labels',\n",
        "    'val_images_dir':'valid/images',\n",
        "    'val_labels_dir':'valid/labels',\n",
        "    'test_images_dir':'test/images',\n",
        "    'test_labels_dir':'test/labels',\n",
        "    'classes': data_yaml['names']\n",
        "}"
      ],
      "metadata": {
        "id": "KJp0H03mOBgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_gradients.datasets.detection import YoloFormatDetectionDataset\n",
        "\n",
        "train_set = YoloFormatDetectionDataset(root_dir=dataset_params['data_dir'],\n",
        "                                       images_dir=dataset_params['train_images_dir'],\n",
        "                                       labels_dir=dataset_params['train_labels_dir'])\n",
        "\n",
        "val_set = YoloFormatDetectionDataset(root_dir=dataset_params['data_dir'],\n",
        "                                     images_dir=dataset_params['val_images_dir'],\n",
        "                                     labels_dir=dataset_params['val_labels_dir'])"
      ],
      "metadata": {
        "id": "7f-p-zoitxZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAGrrq_LN0C0"
      },
      "source": [
        "## üìä Step 3: Perform Analysis\n",
        "\n",
        "Now that you have your dataset loaded and ready, its time to profile it using DataGradients.\n",
        "\n",
        "In this section, you'll use the `DetectionAnalysisManager` class from DataGradients to analyze your dataset. This will trigger feature extraction, visualization, and an interpretation processes provided by DataGradients.\n",
        "\n",
        "**The time it takes to analyze your dataset depends on its size. If your dataset is large, it may take 20 minutes or more.**\n",
        "\n",
        "You instantiate the `DetectionAnalysisManager` with the following arguments:\n",
        "\n",
        "- `report_title`: A title for the analysis report.\n",
        "\n",
        "- `train_data` and `val_data`: The dataloaders for the training and validation sets, respectively.\n",
        "\n",
        "- `class_names`: The list of class names present in the dataset.\n",
        "\n",
        "\n",
        "**üîò There are optional parameters that you can adjust as needed**:\n",
        "\n",
        "- `class_names_to_use`: The subset of class names we want to analyze if you're interested in only certain classes.\n",
        "\n",
        "- `images_extractor` and `labels_extractor`: Custom functions to extract images and labels from the dataset if needed.\n",
        "\n",
        "- `threshold_soft_labels`: A threshold value for soft labels, converting them to hard labels.\n",
        "\n",
        "- `batches_early_stop`: The number of batches to analyze before early stopping.\n",
        "\n",
        "You can find more information about these parameters in the [`DetectionAnalysisManager` class documentation](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/managers/detection_manager.py).\n",
        "\n",
        "Once we have instantiated the `DetectionAnalysisManager`, you can run the analysis by calling the `run()` method.\n",
        "\n",
        "### You'll be prompted for information about your annotations:\n",
        "\n",
        "- Are your annotations labels-first or labels-last?\n",
        "\n",
        "- What format are your annotations in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBVvgtfQYHQb"
      },
      "outputs": [],
      "source": [
        "from data_gradients.managers.detection_manager import DetectionAnalysisManager\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg') # This line is only for Colab\n",
        "\n",
        "analyzer = DetectionAnalysisManager(\n",
        "    report_title=\"Testing Data-Gradients Object Detection\",\n",
        "    train_data=train_set,\n",
        "    val_data=val_set,\n",
        "    class_names=data_yaml['names'],\n",
        ")\n",
        "\n",
        "analyzer.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N6LSinpaPnG"
      },
      "source": [
        "## Step 4: View Full PDF Report\n",
        "\n",
        "When you created the `analyzer`, you passed a value for `report_title`.\n",
        "\n",
        "The report will be saved to a folder in your current working directory that corresponds to that value.\n",
        "\n",
        "If you want to save it in a different folder, you can pass the path to `log_dir` in the `DetectionAnalysisManager` constructor.\n",
        "\n",
        "Inside the log directory, you will find a complete PDF report that summarizes and provides insights on feature extractors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this function was defined and described at the beginning of this notebook\n",
        "display_pdf_pages(\"/content/logs/Testing_Data-Gradients_Object_Detection/Report.pdf\")"
      ],
      "metadata": {
        "id": "JhPzfwooApGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚¨áÔ∏è Download Report\n",
        "\n",
        "Since this is a Google Colab notebook, the report isn't saved to your local drive. If you want to download the report you can do so by running the following cell:"
      ],
      "metadata": {
        "id": "D7ef72LUA133"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjKag797hKv2"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/logs/Testing_Data-Gradients_Object_Detection/Report.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÉ Accessing JSON report.\n",
        "\n",
        "You can also access the json report. This could be helpful to have if you want to save it and analyze changes in your data characteristics over time.\n",
        "\n",
        "Note: I left the below cell commented out because it will make the notebook long and messy."
      ],
      "metadata": {
        "id": "cvx7R-e_Eo3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# report_json = '/content/logs/Testing_Data-Gradients_Object_Detection/summary.json'\n",
        "# print_pretty_json(report_json)"
      ],
      "metadata": {
        "id": "tqM6CTqZCLXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Individual analysis.\n",
        "\n",
        "If you only need one or a few analyses instead of the entire report, you can easily do so with the `run_detection_analysis` function.\n",
        "\n",
        " is a convenient utility that allows you to perform an individual analysis (as opposed to complete report) with ease.\n",
        "\n",
        "It takes various parameters to configure the analysis settings:\n",
        "\n",
        "- `report_title`: The title of the analysis report.\n",
        "- `feature_extractors`: A list of feature extractors to use for analysis.\n",
        "\n",
        "\n",
        "### You can choose from the following options for object detection datasets:\n",
        "\n",
        "#### üîç [ImageColorDistribution](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/common/image_color_distribution.py)\n",
        "\n",
        "This is a comparison between the RGB and grayscale intensity distributions (0-255) of the entire dataset, assuming the RGB channel ordering.\n",
        "\n",
        "It helps to detect differences in image characteristics between the two datasets and potential flaws in the augmentation process.\n",
        "\n",
        "For example, a significant difference in the mean value of a specific color between the two datasets could indicate an issue with augmentation.\n",
        "\n",
        "#### üåì [ImagesAverageBrightness](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/common/image_average_brightness.py)\n",
        "\n",
        "This graph displays how the brightness of each dataset's images is distributed.\n",
        "\n",
        "This can reveal discrepancies between the training and validation sets.\n",
        "\n",
        "For example, it may show that the training set only has daytime images while the validation set only has nighttime images.\n",
        "\n",
        "#### üìè [ImagesResolution](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/common/image_resolution.py)\n",
        "\n",
        "The histograms show how the image height and width are distributed.\n",
        "\n",
        "If any images were resized or added with padding, the histograms will display the size after these modifications.\n",
        "\n",
        "\n",
        "#### üìê [DetectionBoundingBoxArea](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/bounding_boxes_area.py)\n",
        "\n",
        "This graph displays how the bounding box area is distributed among each class.\n",
        "\n",
        "This can reveal any gaps in object size between the training and validation sets, which could negatively impact the model's performance.\n",
        "\n",
        "Having an excessive amount of small objects may suggest that the original image was downsized to an inadequate resolution for the objects in question.\n",
        "\n",
        "#### üî≤ [DetectionBoundingBoxIoU](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py)\n",
        "\n",
        "This chart displays the distribution of Intersection over Union (IoU) values for a given set of boxes.\n",
        "\n",
        "The heatmap indicates the percentage of boxes that overlap with an IoU value within the range of 0 to T for each class.\n",
        "\n",
        "Only the intersection of boxes belonging to the same class are taken into account.\n",
        "\n",
        "#### üìä [DetectionBoundingBoxPerImageCount](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py)\n",
        "\n",
        "These graphs shows how many bounding boxes appear in images.\n",
        "\n",
        "This can typically be valuable to know when you observe a very high number of bounding boxes per image, as some models include a parameter to filter the top k results.\n",
        "\n",
        "#### üìè [DetectionBoundingBoxSize](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py)\n",
        "\n",
        "These heat maps illustrate the distribution of bounding box width and height per class.\n",
        "\n",
        "Large variations in object size can affect the model's ability to accurately recognize objects.\n",
        "\n",
        "#### üî¢ [DetectionClassFrequency](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/classes_frequency.py)\n",
        "\n",
        "One important thing to consider is the frequency of each class in both the training and validation sets.\n",
        "\n",
        "This can reveal any differences in class distribution, such as if a certain class only appears in the validation set.\n",
        "\n",
        "Knowing this in advance can help you understand if your model will be able to accurately predict that class.\n",
        "\n",
        "#### üå°Ô∏è [DetectionClassHeatmap](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py)\n",
        "\n",
        "The heatmap shows where objects are most densely located in the images, which helps you understand how the objects are distributed in the space.\n",
        "\n",
        "By looking at the heatmap, you can easily tell if the objects are mainly concentrated in certain areas or spread out evenly across the scene.\n",
        "\n",
        "This provides useful information to determine if the objects are positioned as expected in the areas of interest.\n",
        "\n",
        "#### üìä [DetectionClassesPerImageCount](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py)\n",
        "\n",
        "The graph displays the frequency of each class in an image and indicates if the number of appearances is consistent or varies across images.\n",
        "\n",
        "#### üëÅÔ∏è [DetectionSampleVisualization](https://github.com/Deci-AI/data-gradients/blob/master/src/data_gradients/feature_extractors/object_detection/sample_visualization.py)\n",
        "\n",
        "The sample visualization feature shows images and labels in a visual format.\n",
        "\n",
        "This helps you better understand the contents of your dataset."
      ],
      "metadata": {
        "id": "3jQiJQfd0avE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_detection_analysis(report_title,\n",
        "                           feature_extractors,\n",
        "                           train_data,\n",
        "                           val_data,\n",
        "                           class_names):\n",
        "    \"\"\"\n",
        "    Run the detection analysis using the provided parameters.\n",
        "\n",
        "    Args:\n",
        "        report_title (str): Title of the analysis report.\n",
        "        feature_extractors (str): Feature extractors to be used.\n",
        "        train_data (str): Training data for the analysis.\n",
        "        val_data (str): Validation data for the analysis.\n",
        "        class_names (list): List of class names for the analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an instance of DetectionAnalysisManager\n",
        "    analyzer = DetectionAnalysisManager(\n",
        "        report_title=report_title,\n",
        "        feature_extractors=feature_extractors,\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        class_names=class_names\n",
        "    )\n",
        "\n",
        "    # Run the analysis\n",
        "    analyzer.run()\n"
      ],
      "metadata": {
        "id": "fEH8o6l5KfSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_detection_analysis(report_title= 'DetectionBoundingBoxPerImageCount',\n",
        "                           feature_extractors='DetectionBoundingBoxPerImageCount',\n",
        "                           train_data=train_set,\n",
        "                           val_data=val_set,\n",
        "                           class_names=data_yaml['names'])"
      ],
      "metadata": {
        "id": "PMCXpEcHSiVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pdf_pages('/content/logs/DetectionBoundingBoxPerImageCount/Report.pdf')"
      ],
      "metadata": {
        "id": "nm5AR_nhS-Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_json = '/content/logs/DetectionBoundingBoxPerImageCount/summary.json'\n",
        "print_pretty_json(report_json)"
      ],
      "metadata": {
        "id": "84dZkCbTT9mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîö You've made it to the end!\n",
        "\n",
        "If you think DataGradients is useful, head over to GitHub and [‚≠êÔ∏è the repo](https://github.com/Deci-AI/data-gradients).\n",
        "\n",
        "\n",
        "### Some datasets you might want to try on your own:\n",
        " - [HuggingFace competition: Ship detection](https://huggingface.co/spaces/competitions/ship-detection)\n",
        "\n",
        "- [Aquarium dataset on RoboFlow](https://public.roboflow.com/object-detection/aquarium)\n",
        "\n",
        "- [Vehicles-OpenImages Dataset on RoboFlow](https://public.roboflow.com/object-detection/vehicles-openimages)\n",
        "\n",
        "- [Winegrape detection](https://github.com/thsant/wgisd)\n",
        "\n",
        "- [Low light object detection](https://github.com/cs-chan/Exclusively-Dark-Image-Dataset)\n",
        "\n",
        "- [Infrafred person detection](https://camel.ece.gatech.edu/)\n",
        "\n",
        "- [Pothole detection](https://www.kaggle.com/datasets/chitholian/annotated-potholes-dataset)\n",
        "\n",
        "- [100k Labeled Road Images | Day, Night](https://www.kaggle.com/datasets/solesensei/solesensei_bdd100k)\n",
        "\n",
        "- [Deep Fashion dataset](https://github.com/switchablenorms/DeepFashion2)\n",
        "\n",
        "- [Playing card detection](https://www.kaggle.com/datasets/luantm/playing-card)\n",
        "\n",
        "- [Anaomoly detection in videos](https://www.crcv.ucf.edu/projects/real-world/)\n",
        "\n",
        "- [Underwater fish recognition](https://www.kaggle.com/datasets/aalborguniversity/brackish-dataset)\n",
        "\n",
        "- [Document layout detection](https://www.primaresearch.org/datasets/Layout_Analysis)\n",
        "\n",
        "- [Trash Annotations in Context](http://tacodataset.org/)\n",
        "\n",
        "\n",
        "# Give [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) a try!\n",
        "Now that you've analyzed your object detection dataset, go and train a YOLO-NAS model on it.\n",
        "\n",
        "[Here's a starter notebook the get you on your way!](https://bit.ly/yolo-nas-starter-notebook)"
      ],
      "metadata": {
        "id": "-e17NJ2v4-09"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}